# Awesome-Visual-Instruction-Tuning

<font size=6><center><big><b> Awesome Visual-Instruction-Tuning [![Awesome](https://awesome.re/badge.svg)](https://awesome.re) </b></big></center></font>

---

# Awesome Datasets





---

# Awesome Papers

|  Title  |   Venue  |   Year   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| [Visual Instruction Tuning](https://arxiv.org/pdf/2304.08485.pdf) | arXiv | 2023 | [GitHub](https://github.com/haotian-liu/LLaVA) | [Demo](https://llava.hliu.cc/) |
| [mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality](https://arxiv.org/pdf/2304.14178.pdf) | arXiv | 2023 | [Github](https://github.com/X-PLUG/mPLUG-Owl) | [Demo](https://www.modelscope.cn/studios/damo/mPLUG-Owl) |
| [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/pdf/2305.06500.pdf) | arXiv | 2023 | [Github](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip) | Local demo |
| [Otter: A Multi-Modal Model with In-Context Instruction Tuning](https://arxiv.org/pdf/2305.03726.pdf) | arXiv | 2023 | [Github](https://github.com/Luodian/Otter) | [Demo](https://otter.cliangyu.com/) | 
| [VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks](https://arxiv.org/pdf/2305.11175.pdf) | arXiv | 2023 | [Github](https://github.com/OpenGVLab/VisionLLM) | [Demo](https://github.com/OpenGVLab/InternGPT) |


